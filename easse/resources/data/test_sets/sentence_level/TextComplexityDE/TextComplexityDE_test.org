Wegen dieser leichten Vergänglichkeit wurde ,Seifenblase zu einer Metapher für etwas, das zwar anziehend, aber dennoch inhalts- und gehaltlos ist.
In der Kunst wird spätestens seit dem Barock die Seifenblase durchgängig ikonographisch als ein Vanitassymbol benutzt und spiegelt sowohl die Schönheit als auch die Flüchtigkeit des menschlichen Lebens wider.
Eine Seifenblase entsteht, wenn sich ein dünner Wasserfilm mit Seifenmolekülen vermischt.
Infolge des gravitationsbedingten Auslaufens (Drainage) der zwischen den Seifenfilmoberflächen befindlichen Flüssigkeit dünnt eine Seifenblase in ihrem oberen Teil zunehmend aus.
Zudem erfolgt im Laufe des Auslaufprozesses eine Anreicherung von Seifenfilm-stabilisierenden Tensidmolekülen im unteren Bereich der Seifenblase, sodass deren obere Region infolge des relativen Mangels von an die Oberfläche adsorbierten Tensidmolekülen zusätzlich destabilisiert wird.
Die Schichtdicke der Seifenblase lässt sich beobachten: Spiegelt die Oberfläche in bunten Interferenzfarben, ist die Schichtdicke vergleichbar mit der Wellenlänge des Lichts.
Die Erzeugung von Seifenblasen ist möglich, da die Oberfläche einer Flüssigkeit  in diesem Falle des Wassers  eine Oberflächenspannung besitzt, die zu einem elastischen Verhalten der Oberfläche führt.
So schildert der achte Gesang beispielsweise, wie im Hause des Phaiakenkönigs Alkinoos Odysseus von den Haussklaven ein Bad zubereitet wird, bevor er sich an der Tafel seines Gastgebers niederlässt.
Vergleichbare Stellen belegen eine sorgfältige Reinigung vor Gebeten und Opfern und die Sitte, dem Gast zunächst Wasser zu reichen, damit er sich seine Hände waschen könne und ihm anschließend ein Bad anzubieten.
Ein bestimmtes Bauschema scheint es nicht gegeben zu haben, man findet Räume mit meist 10 bis 22 Wannen oder Sitzwannen sowohl in rechteckiger (zum Beispiel in Olympia) als auch kreisbogenförmiger Anordnung (zum Beispiel in Gortyn, Gela und Megara Hyblaea).
Es gibt keine Belege, dass Gymnasia vor der römischen Zeit ihren Nutzern bereits heißes Wasser anboten, obwohl dieses zur Entfernung des Öls und Schmutzes besser geeignet gewesen wäre.
Die griechischen Sportstätten mit integrierten Bädern waren allerdings bereits zu diesem Zeitpunkt Begegnungsstätten, in denen man sich stundenlang aufhielt.
Der griechische Komödiendichter Aristophanes (um 450 bis 380 v. Chr.) zweifelt in Die Wolken daran, ob die sich in warmen Bädern verweichlichenden Griechen noch über die Manneskraft verfügten, die ihre Vorfahren in der Schlacht bei Marathon erfolgreich sein ließen.
Noch rigoroser in ihrer Ablehnung waren die Spartaner, die in ihrer Ablehnung warmer Bäder sich auf ihren mythischen Staatsgründer Lykurg beriefen und in ihnen eine Gefahr für die Kriegsfähigkeit ihres Staates sahen.
Außerdem waren bereits zahlreiche Heilquellen bekannt, von denen viele Göttern geweiht waren.
Der Ausdruck Fahrradfahren  auch Radfahren oder Radeln, schweizerisch Velofahren  bezeichnet die Fortbewegung auf einem Fahrrad, sei es als Verkehrsmittel im Radverkehr oder als Sportart, die als Freizeitbeschäftigung, zur Erhaltung der Gesundheit oder als sportlicher Wettkampf bis hin zum Leistungssport betrieben wird.
Die Französin Barbara Buatois fuhr mit einem vollverkleideten Liegerad am 19. Juli 2009 im US-amerikanischen Romeo eine Strecke von 84,02 km, womit sie als erste Frau eine Strecke von mehr als 80 km in einer Stunde mit einem Liegerad zurücklegte.
Der Fahrer hält das System Fahrrad/Fahrer mit kleinen Lenkausschlägen im Gleichgewicht.
Generell gilt Fahrradfahren als sehr gesund (Herz- und Kreislauftraining)[3] und gelenkschonend.
Untersuchungen unter anderem des ADFC haben ergeben, dass die Haltung auf dem sogenannten Reiserad bei richtiger Rahmengröße und richtigem Sattel die Haltung ist, die der menschlichen Anatomie am stärksten entgegenkommt.
Um eine optimale Versorgung der Füße und eine möglichst geringe Belastung der Knie zu erreichen, sollte die Sattelhöhe so eingestellt werden, dass beim Treten der Pedale am tiefsten Punkt das Bein immer noch leicht angewinkelt ist.
Als zweitgrößte Belastungszone ist der Kontakt zwischen Gesäß und Sattel anzusehen.
Dieser Berührungspunkt kann insbesondere daher problematisch werden, da das menschliche Becken mit seinen Sitzbeinhöckern und dem Schambein (bzw. den Schambeinkufen, auch Schambeinkamm genannt) durch den Kippwinkel des Beckens Auswirkungen auf die Biegung der gesamten Wirbelsäule hat.
Somit kann eine falsche Beckenhaltung (z.B. um Missempfindungen aufgrund des Sattels zu vermeiden) zu einer starken Belastung der Wirbelsäule führen.
Gleichfalls problematisch wird ein falscher Beckenwinkel auf dem Sattel für den gesamten Beckenbereich, da dieser Bereich stark mit Blut- und Nervenbahnen durchzogen ist, die auf der Innenseite der Oberschenkel liegen und die die unteren Gliedmaße zu versorgen haben.
Werden diese Versorgungsbahnen durch ein Aufliegen des Schambeins bzw. der Schambeinkufen / des Schambeinkamms auf dem Sattel längere Zeit eingeengt oder gar gequetscht, können dadurch Schädigungen der entsprechenden Gefäße und Versorgungsbahnen entstehen.
Daher ist es notwendig, die Hauptbelastung dieses Kontaktpunktes auf die Sitzhöcker zu bringen und möglichst wenig Druck auf den Bereich der Schambeinkufen, des Damms, der Genitalien und der innenliegenden obersten Teile der Oberschenkel auszuüben.
Dabei spielt die Entfernung des Lenkers zum Gesäß des Radfahrers genauso eine wichtige Rolle wie die Lenkerform (Biegung) und die Höhe des Lenkers, der optimalerweise in gleicher Höhe wie die Mitte der Beckenknochen sein sollte.
Die Hand sollte die gerade Verlängerung des Unterarms bilden und weder nach oben noch nach unten geknickt werden, um so durch das Handgelenk eine permanente und ungehinderte Blutversorgung der gesamten Hand sicherzustellen.
Als grobe Richtlinie sollten die imaginären Verbindungslinien der drei Kontaktpunkte und des Schulterblattes eine Raute darstellen, deren Seiten möglichst etwa gleich lang sein sollten.
Eine Unterteilung wird gleichfalls nach standortbedingter Bauform vorgenommen.
Weitergehende Sicherungsmaßnahmen können eine Videoüberwachung und eine Zugangskontrolle durch einen Türöffner sein, denn viele GAA befinden sich in Vorräumen der Geschäftsstellen der Banken, sodass sie auch außerhalb der Schalteröffnungszeiten zugänglich sind.
Die Softwareausstattung besteht aus einem üblichen Betriebssystem wie beispielsweise Windows XP, Gerätetreibern, einer Kommunikationsschicht (z.B. CEN/XFS oder J/XFS) und einer Anwendung, die den Geldautomaten steuert und die Kommunikation mit der Gegenstelle (Server/Host) organisiert.
Für den barrierefreien Zugang verfügen einige GAA über größere Tasten und Displays sowie über einen Kopfhöreranschluss oder Lautsprecher zur Sprachausgabe, um Menschen mit einer Sehbehinderung die Bedienung zu erleichtern oder überhaupt erst möglich zu machen.
Bei einigen Automaten wurde die Greifhöhe abgesenkt; Tastatur und Ein/Ausgabe-Schächte befinden sich in einem geringeren Abstand zum Boden als bei Standardgeräten.
Der seit dem Produktionsende der Grammophonplatten wesentlich geläufigere Begriff Schellackplatte grenzt diesen älteren Tonträger deutlich von der späteren Schallplatte aus Polyvinylchlorid ab.
Die Signale sind in einer vom Rand der Platte spiralförmig nach innen verlaufenden Rille gespeichert, deren Flanken die Schallschwingung des gespeicherten Signals abbilden.
Bei der Wiedergabe wird die Abtastspitze eines Tonabnehmers entsprechend ausgelenkt.
Die Rückverwandlung in hörbare Schallsignale kann rein mechanisch über eine Membran und einen Schalltrichter oder  bei heute üblichen Plattenspielern  auf elektromechanischem Weg mit anschließender elektronischer Verstärkung erfolgen.
Ihm gelang es 1860, das französische Kinderlied Au clair de la lune mit Hilfe eines großen Trichters einzufangen und mit einer Membran, die die Schwingungen auf eine Schweineborste übertrug, auf eine rußgeschwärzte Walze zu kratzen.
Die Töne wurden zunächst in eine Zinnfolie geritzt, später auf einer Phonographenwalze mit wendelförmiger Tonspur in Höhenschrift gespeichert, wobei das Prinzip der Amplitudenauslenkung auch hier unmittelbar akustisch (Membran/Trichter) genutzt wurde.
Bereits im Jahre 1880 machte der US-amerikanische Physiker Charles Sumner Tainter (Columbia Graphophone Company) die Entdeckung, dass viele technische Nachteile der Edisonischen Walzen (umständliche Handhabung und aufwändige Vervielfältigung) beseitigt werden könnten, wenn man die Tonspur spiralförmig in die Oberfläche einer flachen, runden Scheibe eingraviert.
Als Geschäftsmann sah auch er in der umständlichen  und damit teuren  Vervielfältigung der Walzen den entscheidenden Schwachpunkt des Phonographen und verwendete seine Zeit und Mühe vorrangig auf die Lösung dieses Problems.
Er konstruierte ein Gerät, das die Schallwellen nicht wie bei Edisons Höhenschrift-Phonographen in vertikaler, durch Auf-und-ab-Bewegung des Schneidstichels entstehender Modulation speicherte, sondern die Rille horizontal auslenkte; die mechanischen Schwingungen ließ er eine Stahlnadel schneckenförmig in eine dick mit Ruß überzogene Glasplatte einritzen.
Nach chemischer Härtung des Rußes war er in der Lage, auf galvanoplastischem Wege ein Zink-Positiv und von diesem ein Negativ der Platte anzufertigen, das als Stempel zur Pressung beliebig vieler Positive genutzt werden konnte  die Schallplatte war erfunden.
In den folgenden Monaten entwickelte Berliner in Zusammenarbeit mit dem Techniker Werner Suess sein Verfahren weiter, indem er das rußbeschichtete Glas durch eine mit Wachs überzogene Zink- oder Kupferplatte ersetzte.
Nach der Gravur der Schallrille in die Wachsschicht wurde die Platte einem Säurebad ausgesetzt, das die noch mit Wachs bedeckten Teile der Platte nicht angriff, die freigelegten Rillen aber in das Metall einätzte, so dass nach Entfernung des Wachses eine haltbare metallene Urplatte entstand, die zur Herstellung der Pressmatrizen verwendet werden konnte.
Der zeittypischen Vorliebe für Gräzismen folgend nannte er es Grammophon (sinngemäß: geschriebener Laut).
Zunächst verwendete er als Pressmasse Zelluloid, das er unmittelbar vom Erfinder dieses Werkstoffs, John W. Hyatt, bezog und das sich bald als technisch ungeeignet erwies.
Im Juli 1889 kam Berliner aufgrund materialkundlicher Versuche zu dem Schluss, dass vulkanisiertes Hartgummi als Pressmaterial die günstigsten Eigenschaften aufweise, und erachtete seine Erfindung für ausgereift genug, um den Beginn der Serienproduktion einzuleiten.
Ein Spielwürfel, umgangssprachlich einfach (wie auch ursprünglich) Würfel (von althochdeutsch wurfil: verwandt mit Wurf und werfen[1]), ist ein Gegenstand, der nach einem Wurf auf einer waagerechten Ebene eine von mehreren unterscheidbaren stabilen Ruhelagen einnimmt und in vielen Spielen zum Erzeugen eines zufälligen Symbols (oft einer Zufallszahl) dient.
Die mit Abstand meistverbreiteten Spielwürfel sind jene mit den Ziffern 1 bis 6 oder entsprechend vielen Punkten, den Augen, beschriftete Kuben oder Hexaeder.
Regelmäßige Benutzer unterschiedlicher Würfeltypen bezeichnen diese häufig mit der Abkürzung W, oder auch d für englisch dice (Einzahl auch die),[2] gefolgt von der Angabe der Seitenanzahl, also W6 oder D6 für sechsseitige, W10, W20, W30 für zehn-, zwanzig- und dreißigseitige Spielwürfel.
In Würfelspielen sind Würfel das zentrale Spielelement, es zählen nur der Vergleich der Würfelergebnisse selbst oder direkt mit ihnen zusammenhängendes Taktieren.
Darüber hinaus sind Würfel in einer Vielzahl von Brettspielen bedeutend, um etwa die Bewegungsgeschwindigkeit von Spielfiguren oder den Ausgang von Zufallsereignissen zu bestimmen.
Verwendung finden Würfel in Rollenspielen, bei denen sich in den letzten Jahrzehnten die Verwendung einer Vielzahl weiterer Würfel mit anderen Seitenzahlen durchgesetzt hat, um die Zufallsentscheidungen flexibler und vielfältiger zu gestalten.
Ein eher seltenes, komplett auf Würfel als Spielmaterial setzendes Spielprinzip ist das der Sammelwürfelspiele, bei denen man analog zu Sammelkartenspielen eine Vielzahl von Würfeln käuflich erwerben und taktisch einsetzen muss.
Dabei können die Ergebnisse addiert werden (eine Waffe in einem Rollenspiel richtet soviel Schaden an, wie zwei Würfel zusammen anzeigen) oder als Ensemble betrachtet werden (bei vielen Brettspielen folgen besondere Aktionen, wenn mehrere Würfel die gleiche Zahl zeigen, bei einem sogenannten Pasch).
Um das Werfen mehrerer Würfel zu vereinfachen, Schummeln durch Trickwürfe zu vermeiden oder das Ergebnis vor anderen Spielern zu verbergen, kommen Würfelbecher (Knobelbecher genannt) zum Einsatz.
Um laute Aufprallgeräusche und ein Wegrollen der Würfel zu vermeiden, wird manchmal ein gepolstertes und berandetes Brett (Würfelbrett oder Würfelteller genannt) eingesetzt.
Statt mit ihnen zu würfeln, also Zufallsergebnisse zu erzeugen, können Würfel gezielt auf bestimmte Werte gedreht und so zu deren Anzeige genutzt werden.
Als Zufallsgenerator eingesetzt, wird von einem Würfel üblicherweise eine Gleichverteilung der möglichen Ergebnisse erwartet.
Wenn man von diesen Abweichungen absieht, dann ist Idealität eine Eigenschaft des Bauplans des Würfels, also unter anderem seiner geometrischen Form.
Der Bauplan ist genau dann ideal, wenn die Ruhepositionen des Würfels aufgrund seiner Symmetrie erst durch eine Beschriftung unterscheidbar werden.
Bei einigen Formen kann man versuchen, dies durch die richtige Wahl der Größenverhältnisse auszugleichen, etwa durch Streckung der Seitenflächen beim nebenstehenden Prisma als siebenseitiger Würfel.
Allerdings können die Landewahrscheinlichkeiten neben der Geometrie noch von anderen Bedingungen abhängen, zum Beispiel von der Reibung zwischen Würfel und Unterlage oder  auch unbeabsichtigt  von der Wurftechnik.
Weitere Anforderungen sind, dass der Würfel gut  aber nicht zu lange  rollt und dass die Ruhelagen eine gewisse Stabilität aufweisen.
Beim Casinospiel Craps sowie von einigen Rollenspielern ist dies jedoch verpönt, da ungleichmäßige Abrundungen bestimmte Landeflächen bevorzugen könnten.
Gelegentlich wird die Wahrscheinlichkeitsverteilung bewusst zugunsten bestimmter Ergebnisse manipuliert, möglichst ohne den Würfel optisch zu verändern, um sich im Spiel einen Vorteil zu verschaffen.
Zu stark gezinkte Würfel verraten sich durch eine torkelnde Rollbewegung, was beim Einsatz eines Würfelbechers aber nicht auffällt.
Eine weitere Möglichkeit ist es, im Inneren des Würfels einen Dauermagneten zu platzieren, um den Würfelwurf bei Bedarf durch einen zweiten Magneten, den man z.B. unter die Tischplatte hält, zu beeinflussen.
Seit einigen Jahren finden Rasiermesser jedoch auch zunehmend im Privatbereich wieder eine wachsende Verwendung.
Die Klinge muss vor jeder Rasur auf einem Streichriemen abgeledert und in regelmäßigen Abständen nachgeschliffen werden, um die Schärfe der Schneide zu erhalten.
Die Existenz von Barbieren ist durch Grabszenen belegt, so etwa im Grab des Userhet (KV45), eines hohen Beamten der 18. Dynastie (15501292 v. Chr.).
Die Funktion dieser Messer ist laut Frank Gnegel, Autor einer Kulturgeschichte der Selbstrasur, durch erhaltene Haarreste an den Schneiden eindeutig belegt.
In Pompeji gefundene Exemplare von frühen Klapp-Rasiermessern mit 12 Zentimeter langen trapezförmigen Klingen und Griffen aus Elfenbein gehörten als Luxusobjekte zum Hausstand höherer Schichten.
Seit der Spätantike war die Bartlosigkeit ein Kennzeichen des abendländischen Klerus.
Bei den Mönchsorden regelten genaue Vorschriften die Benutzung und Verwahrung der verwendeten Rasiermesser.
Sie wurden in einem geschlossenen Kasten aufbewahrt und von einem eigens hierfür bestimmten Bruder vor der Verwendung geschärft.
Allerdings wurde das Rasieren nicht durchgängig einheitlich gehandhabt.
Mittelalterliche Bildquellen zeigen sowohl glattrasierte Kleriker als auch solche mit Vollbärten.
Erleichtert wurde der Vorgang allein in Badestuben, in denen Wasser oder Dämpfe das Barthaar vor der Verwendung des Rasiermessers erweichten.
Unter der Vereinbarkeit von Familie und Beruf versteht man seit dem 20. Jahrhundert die Möglichkeit Erwachsener im arbeitsfähigen Alter, sich zugleich Beruf und Karriere einerseits und dem Leben in der Familie und der Betreuung von Kindern und pflegebedürftigen Personen andererseits zu widmen, unter Berücksichtigung der Schwierigkeiten, die dabei auftreten können.
Eine Balance zwischen verschiedenen Lebensbereichen zu ermöglichen, gilt als eine wichtige gesellschaftspolitische Herausforderung, als ein betrieblich relevantes Thema bezüglich Wirtschaftlichkeit und Organisationskultur sowie als ein sozial, kulturell und pädagogisch bedeutsames Thema bezüglich der Gestaltung von Familienkultur.
Wurde die Vereinbarkeit von Familie und Beruf ursprünglich mehr als die Frage angesehen, ob sich Mutterschaft und Berufstätigkeit überhaupt vereinbaren lassen,[1][2] entwickelte sich der gesellschaftliche Diskurs in den Industrienationen im Zuge der Emanzipation in die Richtung, wie sich für Mütter und Väter eine Berufstätigkeit mit der Erziehung der Kinder zeitlich vereinbaren lässt.
Diesem Diskurs liegt die Annahme zugrunde, dass die Eltern jeweils arbeiten wollen oder müssen, dass also die elterliche Berufstätigkeit subjektiv als wertvoll betrachtet wird, etwa weil sie Zufriedenheit gewährt, Sinn stiftet, die soziale Einbindung fördert, die wirtschaftliche Existenz bzw. den Lebensstandard sichert oder weil mehrere dieser Gründe zutreffen.
Angesichts der Veränderung der Altersstruktur und des Anstiegs der Lebenserwartung in vielen Ländern rückt inzwischen auch die Betreuung und Pflege älterer oder pflegebedürftiger Angehöriger stärker in den Mittelpunkt des Interesses, auch der Politik.
Wortprägungen wie weibliche Doppelverdiener[5] wie auch der damals negativ konnotierte Begriff Schlüsselkind wiesen zu dieser Zeit in Westdeutschland auf ungern gesehene Abweichungen vom Frauen- und Familienleitbild.
Vielfach wurde vermutet, der technische Fortschritt des 20. Jahrhunderts werde mehrheitlich zur Verringerung der Arbeitszeit und zu einem Anwachsen der Freizeit führen.
Es wird gesagt, die gegenwärtige Ausrichtung der Gesellschaft und ökonomische Zwänge hätten vielmehr zu einem Anwachsen des Konsums, zu längeren Arbeitszeiten und zu einer Abwertung des Lebensbereichs Familie geführt.
Studien belegen, dass die Frage, ob Kinder aus der Situation einen Nachteil oder auch einen Vorteil beziehen, nicht mit Ja oder Nein beantwortet werden kann: Die Wirkung der Berufstätigkeit auf das Kind hängt von Kontextfaktoren ab, insbesondere vom Berufskontext, von der Art der Verwendung von Zeit und Geld, von der Qualität der nichtelterlichen Kinderbetreuung und von der Zufriedenheit der Frau mit ihrer Rolle.
Teilweise vertreten verschiedene gesellschaftliche Gruppen jeweils den Standpunkt der Wahlfreiheit, allerdings mit unterschiedlicher Gewichtung: Die eine Seite hebt die Möglichkeit zur Erwerbsarbeit auch mit Kindern hervor, die andere betont die Freiheit, auch die traditionelle Familienform zu wählen.
In Deutschland werden Infrastrukturmaßnahmen in Kombination mit Änderungen der familienbezogenen Transferleistungen und der Besteuerung insbesondere als wesentlich für eine Verringerung der Kinder- und Familienarmut genannt.
Diese Erwartung wird im Zusammenhang mit der hohen Scheidungsrate, den sich ändernden Regelungen zum Unterhalt und der Diskussion um eventuelle Änderungen der Witwen-/Witwerrente in verstärktem Maß auch von der Gesellschaft an sie herangetragen.
Die Pluralisierung der Familienformen mit zunehmender Zahl von Patchwork- und Einelternfamilien erfordert gesellschaftliche Anpassungen, um eine finanzielle Überforderung der Unterhalt zahlenden Eltern beziehungsweise der Sozialsysteme zu vermeiden und zugleich allen Personen einen angemessenen Lebensunterhalt zu sichern.
Dies bezieht sich auf die spätere Altersrente, aber auch auf Fälle von Arbeitslosigkeit, Arbeitsunfähigkeit oder Trennung, denn bei Erwerbstätigkeit beider Partner besteht eine geringere Abhängigkeit von staatlicher Unterstützung oder Unterhaltszahlungen.
In vielen Familien ist es zudem ökonomisch kaum möglich, dass sich ein Elternteil ganz der Haus- und Familienarbeit widmet  für eine zunehmende Zahl von Haushalten reicht Anfang des 21. Jahrhunderts ein Erwerbseinkommen allein nicht mehr zum Unterhalt einer Familie aus.
Als Gründe dafür, dass Väter ihre Arbeitszeit relativ selten für die Familienarbeit reduzieren, werden u. a. finanzielle Nachteile aufgrund von Gehaltsunterschieden zwischen Männern und Frauen, fehlende Teilzeitstellen für höhere Positionen sowie eine Profitorientierung der Konzerne, die auf familiäre Bedürfnisse der Angestellten keine Rücksicht nehme, genannt.
Eine Retraditionalisierung der Rollen findet Studien zufolge oft nach der Geburt des ersten Kindes statt: selbst bei vorher weitgehend egalitärem Rollenverständnis beider Partner werden nach der Geburt vor allem die Auffassungen der Männer wieder traditioneller, während die der Frauen egalitär bleiben; dies führe oft zu Spannungen in der Partnerschaft.
Globale Erwärmung ist der beobachtete und prognostizierte Trend zu einer im Vergleich zu den vorindustriellen Werten höheren globalen Durchschnittstemperatur mit Folgen wie steigenden Meeresspiegeln, Gletscherschmelze, Verschiebung von Klimazonen, Vegetationszonen und Lebensräumen, verändertes Auftreten von Niederschlägen, stärkere oder häufigere Wetterextreme wie Überschwemmungen, Stürme und Dürren, Ausbreitung von Parasiten und tropischen Krankheiten sowie mehr Umweltflüchtlingen.
Während über die Ursachen der globalen Erwärmung weitgehend Einigkeit besteht[1] (hauptsächlich menschliche Emissionen von Treibhausgasen), werden ihre Folgen intensiv erörtert.
Nach einer Studie des Stockholm Resilience Centre von 2009 ist der ermittelte Grenzwert für den Kohlendioxidgehalt der Atmosphäre bereits um 11 % überschritten, so dass der anthropogene Klimawandel nach dem Artensterben das zweitgrößte globale ökologische Problem darstellt; er ist dabei auch ein wesentliches Merkmal des Anthropozäns sowie eine der Folgen der zunehmenden Hemerobie.
Über die möglichen Folgen der Erwärmungen informieren auch die Unterartikel Folgen der globalen Erwärmung in Deutschland, Folgen der globalen Erwärmung in Europa, Folgen der globalen Erwärmung in der Arktis, Folgen der globalen Erwärmung in der Antarktis sowie Folgen der globalen Erwärmung für den Weinbau.
In welchem Ausmaß die Durchschnittstemperatur im Laufe des 21. Jahrhunderts ansteigt, hängt insbesondere von der Menge an Treibhausgasen ab, die ausgestoßen werden.
Dazu gehören steigende Meeresspiegel, Gletscherschmelze oder statistisch signifikante Abweichungen vom gewöhnlichen Wettergeschehen (siehe unten).
Die Klimamodelle beschreiben derzeit auf globaler Ebene die Folgen recht gut, können diese jedoch auf regionaler Ebene nur recht unsicher abschätzen.
Falls er in sehr kurzer Zeit erfolgen sollte, werden sowohl die ökonomischen Anpassungskosten als auch die Einflüsse auf die Natur voraussichtlich drastisch spürbar sein.
Dem IPCC zufolge weisen von 29 436 Serien mit Beobachtungsdaten aus 75 Studien, die signifikante Veränderungen in physikalischen oder biologischen Systemen aufzeigen, 89 % mit den Erwartungen über eine erwärmte Welt übereinstimmende Veränderungen auf.
Mit über 28.000 Datensätzen zu biologischen Veränderungen ist Europa hierbei deutlich überrepräsentiert, doch dass hiervon 90 % eine mit der Erwärmung übereinstimmende Veränderung anzeigen macht das Ergebnis auch sehr robust.
Infolge der Eisschmelze in der Arktis und dem damit einhergehenden Verlust an beschwerender Masse steigt beispielsweise die Landmasse (-> Landfläche) Skandinaviens in Finnland und Schweden schneller als der regionale Meeresspiegel.
Den Hafen des schwedischen Luleå können viele Schiffe nicht mehr anlaufen, hier sinkt der Meeresspiegel jährlich ebenfalls um nahezu einen Zentimeter, prognostiziert für noch mindestens 600 weitere Jahre.
Werden keine Maßnahmen zur Bekämpfung des Klimawandels getroffen, sind weltweit 16 % aller Arten vom Aussterben bedroht, wie eine 2015 in Science erschienene Übersichtsarbeit ergab.
Laut dem vom Arktischen Rat in Auftrag gegebenen Arctic Climate Impact Assessment wird in zahlreichen polaren Gebieten die Artenvielfalt zunehmen, weil im Zuge der Erwärmung neue Arten in die Arktis einwandern werden und die Gesamtzahl der Arten und deren Produktivität zunehmen wird.
Würden die Meere kein Kohlendioxid lösen, läge die atmosphärische Konzentration von Kohlenstoffdioxid einer Untersuchung aus dem Jahre 2004 zufolge um 55 ppm höher, zum damaligen Zeitpunkt also statt bei 380 ppm bei wenigstens 435 ppm.
Verschiedene Effekte sorgen jedoch dafür, dass mit steigenden Temperaturen und wachsendem atmosphärischem CO2-Anteil die Aufnahmefähigkeit der Meere für Kohlenstoff abnimmt.
Wie weit die Aufnahmefähigkeit sinkt, lässt sich schwer beziffern.
Der mögliche Kollaps von Teilen des antarktischen Eisschildes[13][14] ist in diesen Berechnungen noch eingeschlossen und würde zu massiven zusätzlichen Erhöhungen führen.
Für die Meeresspiegelerhöhung werden im Wesentlichen zwei Faktoren verantwortlich gemacht: Zum einen dehnt sich das Meerwasser bei höheren Temperaturen stärker aus, zum anderen kommt es bei höheren Temperaturen zum verstärkten Abschmelzen von Gletschern (siehe unten).
Die Folgen dieser Versauerung betreffen zunächst kalkskelettbildende Lebewesen, deren Fähigkeit, sich Schutzhüllen bzw. Innenskelette zu bilden, bei sinkendem pH-Wert nachlässt.
Der pH-Wert ist für ideal verdünnte Lösungen definiert und daher auf das salzhaltige Meereswasser nicht direkt anwendbar.
Um Durchschnittswerte für Meereswasser angeben zu können, müssen darüber hinaus Modelle angewendet werden, um ein chemisches Gleichgewicht des Ozeans zu simulieren.
Die wichtigsten Ursachen für diese Differenz um 0,25 Einheiten sind die Temperatur des Wassers, der lokale Auftrieb von kohlenstoffdioxidreichem Tiefenwasser, sowie die biologische Produktivität, die dort, wo sie hoch ist, in Form von Meereslebewesen viel Kohlenstoffdioxid bindet und in tiefere Wasserschichten transportiert.
Aus der isotopischen Zusammensetzung von Borhydroxiden lässt sich bestimmen, dass der pH-Wert an der Meeresoberfläche vor etwa 21 Millionen Jahren etwa 7,4 ± 0,2 betrug, bis er vor ungefähr 7,5 Millionen Jahren auf den Wert von 8,2 ± 0,2 stieg.
Die Versauerung verläuft nach dem Fünften Sachstandsbericht des IPCC schneller als alle ähnlichen Versauerungen der vergangenen 65 Mio. Jahre, eventuell der vergangenen 300 Mio. Jahre.
Einer 2005 erschienenen Studie der Stanford University zufolge, die einen vorindustriellen pH-Wert des oberflächennahen Meerwassers von durchschnittlich 8,25 annimmt, verringerte sich der pH-Wert durch die Aufnahme von Kohlenstoffdioxid auf den damaligen Wert von durchschnittlich 8,14.
In beiden Fällen wird die Versauerung auf die menschlichen Emissionen von Kohlenstoffdioxid zurückgeführt und mit 0,11 pH-Einheiten beziffert.
Eine Versauerung erfolgt auch in Küsten- oder Schiffsnähe durch Säureeinträge verursacht durch Schwefeloxide und Stickoxide (siehe Saurer Regen).
Bei Seamounts, an den Kontinentalhängen und in Flachmeeren (zum Beispiel in Teilen des Weddell-Meeres)[10] kann das anthropogene CO2 bereits bis zum Meeresboden gelangen.
Betroffen sind davon bis auf wenige Ausnahmen alle Regionen, von den Tropen über die mittleren Breiten bis zu den polaren Eiskappen.
Ebenso zu beobachten ist ein Rückgang des Eises in den polaren Gebieten, wo es in den zurückliegenden Jahren vermehrt zum Abbrechen größerer Schelfeise gekommen ist.
Dieser in den 1980er- und 1990er-Jahren kurzzeitig bestehende, auf örtlich veränderte Niederschlagsmuster zurückgehende Trend hat sich allerdings etwa seit dem Jahr 2000 zumindest in den ersten beiden Regionen entweder wieder umgekehrt oder ist zumindest deutlich abgeflacht.
Eine indirekte Wirkung des anthropogenen Klimawandels ist eine veränderte Verteilung von Niederschlägen, die ebenfalls die Massenbilanz von Gletschern beeinflussen kann.
Die Folgen des Phänomens bergen erhebliche Risiken für einen momentan nur schwer abschätzbaren Anteil der gegenwärtigen und künftigen Weltbevölkerung.
Der zunehmende Abfluss des Gletscherwassers führt zudem zum globalen Anstieg des Meeresspiegels und bedroht damit auch nicht unmittelbar im Einflussbereich von Gletschern lebende Menschen.
Entscheidend für das Fortbestehen eines Gletschers ist seine Massenbilanz, die Differenz von Akkumulation (wie Schneefall, Ablagerung von Triebschnee und Lawinen, Kondensation von atmosphärischem Wasserdampf und Anfrieren von Regenwasser) und Ablation (Schmelze, Sublimation sowie Abbruch von Lawinen).
Im Zehrgebiet (Ablationsgebiet) dagegen überwiegt die Ablation gegenüber dem Nachschub durch Schnee.
Bei einem Klimawandel können sich sowohl Lufttemperaturen als auch der Niederschlag in Form von Schnee verändern und damit die Massenbilanz verschieben.
Nach dem 2007 erschienenen Vierten Sachstandsbericht der Zwischenstaatlichen Sachverständigengruppe über Klimaänderungen (IPCC) stieg die weltweite durchschnittliche Lufttemperatur in Bodennähe zwischen 1906 und 2005 um 0,74 °C (± 0,18 °C) an.
Deshalb ist für jede der betroffenen Regionen gesondert zu prüfen, welche Faktoren für den Rückgang der Gletscher ursächlich und gegebenenfalls dominierend sind.
Kryokonit ist ein dunkler biogener Oberflächenstaub auf Schnee und Eis, der durch Winde in der Atmosphäre über weite Strecken transportiert wird und gewöhnlich auf Gletschern weltweit zu beobachten ist.
Wegen seiner dunklen Färbung reduziert Kryokonit wesentlich die Oberflächenreflexion des Sonnenlichts und beschleunigt oder initiiert damit das Schmelzen der Gletscher.
Dieses organische Material besteht zum Teil aus photosynthetisch aktiven Mikroorganismen wie Cyanobakterien oder auch Bärtierchen,[13] wie es am Rotmoosferner nachgewiesen wurde.
Dadurch nimmt die Gletscherfläche im Zehrgebiet, dort ist die Ablation am höchsten, zu.
Auf eine Klimaerwärmung wie die globale Erwärmung oder eine Abnahme des Schneefalls, die zu einer negativen Massenbilanz führen, reagiert der Gletscher mit einem Rückgang.
Letztere verzeichnet auch die verschiedenen Wetteranomalien in historischer Zeit, die unter anderem von heftigen vulkanischen Eruptionen hervorgerufen wurden.
Zuverlässige und instrumentell ermittelte Temperatur- und Klimadaten stehen auf breiterer Basis erst seit der zweiten Hälfte des 19. Jahrhunderts zur Verfügung.
Zusätzlich kommt in der Forschung ein breites Spektrum verschiedener Isotopenanalysen zum Einsatz, deren jüngste Entwicklungen eine bis vor kurzem unerreichbare Messgenauigkeit ermöglichen.
Allerdings deckt die 14C-Methode nur einen relativ schmalen zeitlichen Bereich von 300 bis maximal 57.000 Jahren ab.
In letzter Zeit kommt die 40Ar/39Ar-Datierung verstärkt zum Einsatz, da diese Methode auf der Grundlage des Edelgases Argon erheblich präzisere Ergebnisse als die herkömmliche Kalium-Argon-Datierung ermöglicht.
So gab es noch keine Meere, Niederschläge oder sonstiges flüssiges Wasser auf der Erde, und die Zusammensetzung der reduzierenden Uratmosphäre unterschied sich stark von der heutigen Erdatmosphäre.
Vor 2,6 Milliarden Jahren bildete sich im Laufe der Entwicklung der Erdatmosphäre durch die Aktivität von Cyanobakterien der erste Sauerstoff in der Uratmosphäre und erreichte vor circa 2,2 Milliarden Jahren signifikante Konzentrationen.
Die Veränderung der Konzentration der Klimagase und ihrer Zusammensetzung veränderte zudem den Strahlungshaushalt der Erde und brachte den Treibhauseffekt in Gang, der die Erde seitdem erwärmt.
Sie umfasst die in der Erde gespeicherte Energie, soweit sie entzogen und genutzt werden kann, und zählt zu den regenerativen Energien.
Sie kann sowohl direkt genutzt werden, etwa zum Heizen und Kühlen im Wärmemarkt (Wärmepumpenheizung), als auch zur Erzeugung von elektrischem Strom oder in einer Kraft-Wärme-Kopplung.
Dieser Temperaturgradient ist mit etwa 1 K/km viel zu klein, als dass Wärmeleitung einen wesentlichen Beitrag zum Wärmetransport leisten könnte.
Die im Vergleich zum Erdalter sehr rasche Konvektion  die ozeanische Kruste wurde und wird selten älter als 100 Millionen Jahre  wäre ohne Wärmequellen bald zum Erliegen gekommen.
Das ist nur etwa das Doppelte des Weltenergiebedarfs, was bedeutet, dass Erdwärmenutzung im großen Stil immer auf eine lokale Abkühlung des Gesteins hinausläuft.
Aufgrund der Wärmekapazität des Gesteins, und der damit verbundenen Menge der gespeicherten Wärme kann aber bei ausreichend großem Volumen die Abkühlung innerhalb der Nutzungsdauer gering bleiben und die Erdwärmenutzung somit nachhaltig sein.
Diese finden sich beispielsweise in Grabenbrüchen (in Deutschland der Oberrheingraben) oder in tiefen Sedimentbecken.
Solche Gebiete sind zunächst Gebieten vorzuziehen, in denen ein dichtes Gestein für die Konvektion erst erschlossen werden muss.
Dies sind geologische Wärmeanomalien, die oft mit aktivem Magmatismus einhergehen; dort sind mehrere hundert Grad heiße Fluide (Wasser/Dampf) in einer Tiefe von wenigen hundert Metern anzutreffen.
Abhängig von den Druck- und Temperaturbedingungen können Hochenthalpie-Lagerstätten mehr dampf- oder mehr wasserdominiert sein.
Hierfür wird das im Untergrund erhitzte Wasser genutzt, um eine Dampfturbine anzutreiben. Der geschlossene Kreislauf im Zirkulationssystem steht so unter Druck, dass ein Sieden des eingepressten Wassers verhindert wird und der Dampf erst an der Turbine entsteht (Flash-Verdampfung).
In der Regel sind jedoch tiefe Bohrungen notwendig; für die Stromerzeugung sind Temperaturen über 80 °C erforderlich.
Generell werden im Bereich der tiefen Geothermie drei Arten der Wärmeentnahme aus dem Untergrund unterschieden; welches der in Frage kommenden Verfahren zum Einsatz kommt, ist von den jeweiligen geologischen Voraussetzungen, von der benötigten Energiemenge sowie dem geforderten Temperaturniveau der Wärmenutzung abhängig.
HDR-Verfahren befinden sich in den Pilotprojekten in Bad Urach (D), in Soultz-sous-Forêts im Elsass (F) und in Basel (CH) in der Erprobung.
Er wurde am 11. Mai 1910 unter Schutz gestellt, wird vom National Park Service verwaltet und dient wegen seiner langen Forschungsgeschichte als Referenzgebiet für die Erforschung der Klimageschichte und der globalen Erwärmung.
Beide Parks zusammen wurden 1932 als weltweit erstes grenzüberschreitendes Naturschutzgebiet unter dem Namen Waterton-Glacier International Peace Park zu einem Internationalen Friedenspark ernannt und 1995 durch die UNESCO zum Weltnaturerbe erklärt.
Der Glacier-Nationalpark bezieht seinen Namen von der durch Vergletscherung während des Eiszeitalters geprägten Landschaft.
Der Berg ist der Wasserscheidepunkt an dessen Flanken sich die Einzugsgebiete des Pazifischen Ozeans, des Atlantischen Ozeans über den Golf von Mexiko und des Arktischen Ozeans über die Hudson Bay berühren.
In den Tieflagen liegen Zungenbeckenseen, im höheren Gelände handelt es sich um Karseen.
Der Westen unterliegt dem maritimen Einfluss des Pazifischen Ozeans mit gemäßigten Temperaturen und hohen Niederschlägen, während die Ostseite dem kontinentalen Klima zugehörig ist, das durch extreme jahreszeitliche Temperaturunterschiede und die für Nordamerika typischen Blizzards aus nördlichen Richtungen geprägt ist.
Spannungen innerhalb der Decke führten zu einer Synklinale, einer konkav  also nach innen  gewölbten Struktur, durch die Gesteinsschichten im Osten und Westen des Parks höher liegen als im Zentrum.
Aus den Ablagerungen von Sanden, Tonen und den Kalkgehäusen von Zooplankton in einem Urmeer entstanden zunächst Gesteine wie Sandstein, Schiefer und Kalkstein.
Teile davon wurden über geologische Zeiträume durch Druck späterer Schichten zu Metamorphen Gesteinen wie Quarzit, Tonschiefer sowie kristallinem Kalkstein (Marmor) und Dolomit umgewandelt.
In der Appekunny Formation im Osten des Parks, die auf ein Alter von 1,51,3 Milliarden Jahre datiert wird, wurden 1982 Abdrücke gefunden, die von den Entdeckern als Metazoa interpretiert und nach neuen Untersuchungen 2002 als Horodyskia moniliformis beschrieben wurden.
Sie gehören zu den frühesten Spuren vielzelliger Tiere weltweit.
Beim Recycling, Rezyklierung bzw. Müllverwertung werden Abfallprodukte wiederverwertet bzw. deren Ausgangsmaterialien werden zu Sekundärrohstoffen.
Ein möglicher Nachteil von beispielsweise Kunststoff ist, dass  bei vertretbarem Aufwand  das Material nicht mehr die ursprüngliche Qualität oder dessen Verarbeitbarkeit erreicht wie bei der Primärherstellung vor dem Recyclingprozess.
Diese Abwertung wird auch als Downcycling bezeichnet, während beim Upcycling aus Abfallstoffen eines Prozesses hochwertigere Produkte hergestellt werden können.
Es kommt hierbei auf die Qualität und Sortenreinheit der gesammelten Altteile und den Aufbereitungsprozess und die Nachadditivierung an.
Auch der Gesamtenergieverbrauch bei der Wiederaufbereitung wird vielfach überschätzt.
Diese vollständige Wiederverwertung ist Basis der Subsistenzwirtschaft.
Im Mittelalter verfiel diese Organisation größtenteils  Exkremente und Abfälle wurden teilweise einfach nur auf die Straße gekippt und allenfalls von Haustieren verwertet.
Erst mit Aufkommen der grünen Bewegung in den 1970/80er-Jahren fand ein Umdenken statt, dass Müllentsorgung einer der Hauptfaktoren der Umweltverschmutzung darstellt.
Ausgehend von Altpapier-Wiederverwendung wurden zunehmend Technologien erarbeitet, die die Wiederaufbereitung aller Arten von Altstoffen wirtschaftlich machen, wodurch Abfall zu einem bedeutenden Wirtschaftsgut wurde: Geprägt wurde dafür der Ausdruck Sekundärrohstoff.
Besonders in Zeiten der Kriegswirtschaft wird auf Metallgegenstände des zivilen Gebrauches zurückgegriffen zwecks Sekundär-Rohstoffgewinnung zur Waffenproduktion, wie etwa 1940 unter dem Motto Metallspende des deutschen Volkes.
Zuvor hatte er bereits mehrere Regierungsämter bekleidet, unter anderem das des Innenministers, des Ersten Lords der Admiralität und des Schatzkanzlers.
Erst bei Ausbruch des Zweiten Weltkriegs 1939 kehrte Churchill, der als erklärter Gegner Hitlers bekannt war, in die Regierung zurück, zunächst erneut als Erster Lord der Admiralität.
Mit seiner Weigerung, in Verhandlungen mit Hitler einzutreten, und mit seinen Reden stärkte er in den kritischen Monaten des Frühjahrs und Sommers 1940 den britischen Widerstandswillen.
Seinen Wahlkreis Woodford im Nordosten Londons vertrat er bis 1964, ein Jahr vor seinem Tod, im Unterhaus.
Das autoritäre Erziehungssystem dort widerstrebte ihm, und er blieb mehrfach sitzen.
Zwischen 1895 und 1901 nahm Churchill als aktiver Soldat und Kriegsberichterstatter an fünf verschiedenen Kolonialkriegen teil, unter anderem in Kuba auf Seiten der Spanier während des dortigen Unabhängigkeitskrieges und in verschiedenen Teilen des Empire, etwa in Malakand in der Nordwestlichen Grenzprovinz Britisch-Indiens.
Dabei ritt er in der Schlacht von Omdurman eine der letzten großen Kavallerieattacken der britischen Militärgeschichte mit.
Seinem Biographen Martin Gilbert zufolge war der Vertrag, den Churchill mit der Zeitung aushandelte, wahrscheinlich der günstigste Vertrag, den überhaupt ein Kriegsberichterstatter bis dahin abgeschlossen hatte.
Die Ursache für die Vergabe durch ein norwegisches Gremium liegt vermutlich darin, dass zu Nobels Lebzeiten Schweden und Norwegen vereinigt waren und außenpolitische Fragen nur durch das schwedische Parlament entschieden wurden.
Man geht allerdings davon aus, dass er der Meinung war, das norwegische Parlament, das nur für die Innenpolitik verantwortlich war, wäre Manipulationen durch die Regierung weniger stark ausgesetzt.
1977 wurde die Regel insofern noch einmal verschärft, dass keine Mitglieder aus regierungsnahen Ausschüssen zugelassen werden, gleichzeitig mit der Namensänderung von Nobel-Komitee des norwegischen Parlamentes in Norwegisches Nobel-Komitee.
Die ausgewählten Personen und Organisationen wirken häufig stark polarisierend, und es kommt bei nahezu jeder Vergabe zu Anfeindungen über die Entscheidung.
Eine Rücknahme des Preises ist jedoch nicht möglich und die Entscheidung des Gremiums entsprechend nicht formal anfechtbar.
Das Komitee zog eine posthume Vergabe in Betracht und prüfte sie.
Unter der Herrschaft des Vereinigten Königreichs vereinte es Dominions, Kronkolonien, Protektorate, Mandatsgebiete und sonstige abhängige Gebiete, die aus den englischen Überseebesitzungen, Handelsposten und Strafkolonien hervorgegangen waren.
Die Abspaltung der Dreizehn Kolonien nach dem Amerikanischen Unabhängigkeitskrieg (17751783) bedeutete zwar den Verlust der bevölkerungsreichsten Überseegebiete, doch wandte sich Großbritannien bald Afrika, Asien und Ozeanien zu.
Mehrere Siedlerkolonien, deren Bevölkerung vor allem durch den stetigen Zustrom von Auswanderern aus dem Mutterland zunahm, erhielten mit der Zeit mehr Autonomie und wurden zu Dominions erhoben.
Damals kaufte die konservative Regierung Disraeli für 4 Millionen Pfund die Aktienanteile des ägyptischen Herrschers Ismail an der Sueskanal-Gesellschaft auf, um diesen strategisch wichtigen Handelsweg nach Indien zu sichern.
Die Rivalität zu Russland (vgl.: The Great Game), die im Krimkrieg (18541856) eine erste Eskalation erfahren hatte, und die Angst vor einer russischen Expansion in Richtung Süden und Indien war ein weiterer Faktor der britischen Politik.
Wegen des wachsenden Einflusses des Deutschen Reiches und der Vereinigten Staaten büßte Großbritannien seit etwa 1900 zunehmend seine politische und wirtschaftliche Vormachtstellung ein.
Wirtschaftliche und politische Spannungen mit dem Deutschen Reich gehören zu den wichtigsten Ursachen des Ersten Weltkriegs, in dem Großbritannien in hohem Maße auf die Unterstützung durch seine Kolonien angewiesen war.
Zwar erreichte Großbritannien nach Kriegsende 1918 durch die Übernahme deutscher Kolonien seine größte Ausdehnung, doch leiteten finanzielle Probleme und zunehmende Autonomiebestrebungen das Ende seiner globalen Bedeutung ein.
Darüber unterstehen 14 kleinere Überseegebiete weiterhin der britischen Souveränität.
Premierminister David Lloyd George honorierte diesen wichtigen Beitrag, indem er 1917 mit den Premierministern der Dominions das Reichskriegskabinett (Imperial War Cabinet) bildete, um die gemeinsamen Anstrengungen zu koordinieren.
Seine Forschungen zur Struktur von Materie, Raum und Zeit sowie zum Wesen der Gravitation veränderten maßgeblich das zuvor geltende newtonsche Weltbild.
Sein Vater Hermann Einstein stammte aus der oberschwäbischen Kleinstadt Buchau, in der es seit dem Mittelalter innerhalb des Territoriums des freiweltlichen Damenstifts Buchau eine bedeutende jüdische Gemeinde gab (siehe auch: Familie Einstein in Bad Buchau).
Der erste namentlich nachgewiesene Vorfahre Albert Einsteins, ein aus dem Bodenseeraum stammender Pferde- und Tuchhändler namens Baruch Moses Ainstein, wurde im 17. Jahrhundert in die Gemeinde aufgenommen.
In der Schule war er ein aufgeweckter, bisweilen gar aufrührerischer Schüler.
Er sah vor, die deutsche Hochseeflotte trotz der bereits feststehenden Kriegsniederlage Deutschlands in eine letzte Schlacht gegen die britische Royal Navy zu entsenden.
Über die Parlamentarisierung hinausgehende, von rätedemokratischen Ideen geleitete Ziele des linken Flügels der Revolutionäre scheiterten am Widerstand der SPD-Führung.
Der Flottenbefehl vom 24. Oktober 1918 und die Vorbereitungen zum Auslaufen lösten zunächst eine Meuterei unter den betroffenen Matrosen und dann eine allgemeine Revolution aus, die in wenigen Tagen die Monarchie im Reich beseitigte.
Zudem waren sie überzeugt, im Sinne der neuen Regierung zu handeln, die Friedensverhandlungen mit der Entente anstrebte.
Deren Glaubwürdigkeit hätte ein gleichzeitiger Angriff der Flotte zunichtegemacht.
Vielmehr bezeichnete er das Vorhaben der Admiralität als eine Meuterei der Flottenführung gegen die Regierung und ihre Politik.
Der Matrosenaufstand begann auf Schillig-Reede vor Wilhelmshaven, wo die deutsche Hochseeflotte in Erwartung der geplanten Seeschlacht vor Anker gegangen war.
Auf drei Schiffen des III. Geschwaders weigerten sich die Matrosen, die Anker zu lichten.
Auf den Schlachtschiffen des I. Geschwaders Thüringen und Helgoland gingen Teile der Besatzungen zu offener Meuterei und Sabotageakten über.
Als aber am 31. Oktober einige Torpedoboote ihre Geschütze auf diese Schiffe richteten, verschanzten sich rund 200 Meuterer zunächst unter Deck, ließen sich dann aber widerstandslos verhaften.
Da die Marineleitung sich des Gehorsams der Mannschaften nicht mehr sicher war, ließ sie ihren Schlachtplan fallen und beorderte das Geschwader nach Kiel zurück.
Nachdem die Polizei das Gewerkschaftshaus für den 2. November gesperrt hatte, versammelten sich am Folgetag mehrere tausend Matrosen und Vertreter der Arbeiter nachmittags auf dem Großen Exerzierplatz.
Sie kehrten entweder um oder schlossen sich der Aufstandsbewegung an.
Seit der Gründung des Parlaments 1952 wurden seine Kompetenzen bei der EU-Rechtsetzung mehrmals deutlich erweitert, vor allem durch den Vertrag von Maastricht 1992 und zuletzt durch den Vertrag von Lissabon 2007, der am 1. Dezember 2009 in Kraft trat.
Anders als in den meisten nationalen Parlamenten, wo die Regierungsfraktionen normalerweise loyal zur Regierung stehen und deren Gesetzentwürfe prinzipiell unterstützen, bilden sich im Europäischen Parlament je nach Abstimmungsthema wechselnde Mehrheiten.
Dies bewirkt auch, dass die einzelnen Europa-Abgeordneten unabhängiger sind und mit Verhandlungsgeschick und Sachkenntnis größeren Einfluss auf die EU-Gesetzgebung haben, als es Abgeordneten nationaler Parlamente möglich ist.
Das Bundesverfassungsgericht spricht dem Europäischen Parlament in seinem Urteil zum Lissabon-Vertrag vom 30. Juni 2009 nur eine eingeschränkte demokratische Legitimation zu und sieht seine Entscheidungskompetenzen bezüglich weiterer Schritte einer europäischen Integration dadurch begrenzt.
In ihren Heimatländern sind diese Abgeordneten Mitglieder in rund 160 verschiedenen nationalen Parteien, die sich auf europäischer Ebene großenteils zu Europaparteien zusammengeschlossen haben.
Demzufolge wird das Parlament gemeinsam mit dem Rat als Gesetzgeber tätig, übt gemeinsam mit ihm die Haushaltsbefugnisse aus und nimmt die politische Kontrolle wahr.
Auch um den hohen Zeitaufwand dieses Verfahrens zu umgehen, werden jedoch immer mehr Gesetzesvorschläge in informellen Trilogverfahren verhandelt, um dann bereits in erster Lesung beschlossen werden zu können: zwischen 2004 und 2009 etwa traf dies auf 72 % aller Gesetzesentwürfe zu, im Vergleich zu 33 % zwischen 1999 und 2004.
In einer verbindlichen Erklärung aus dem Jahr 2010 haben sich die Parlamentarier mit der Kommission geeinigt, den geltenden europarechtlichen Vorschriften eine Interpretationshilfe zu geben, sodass in Zukunft auf Anstoß des Parlamentes die Kommission innerhalb von zwölf Monaten einen Gesetzentwurf vorlegen oder innerhalb von drei Monaten detailliert begründen muss, warum sie es nicht macht.
Neben dem ordentlichen Gesetzgebungsverfahren gibt es noch andere Formen der Rechtsetzung in der EU, bei denen das Parlament weniger Mitspracherechte besitzt.
Dezember 2009 besitzt das Europäische Parlament im Bereich der Gemeinsamen Handelspolitik das Recht, Abänderungsvorschläge zu Gesetzesentwürfen einzubringen sowie auf Ablehnung des jeweiligen Rechtsaktes.
Die Geschichte der Europäischen Union ist durch ein Geflecht konkurrierender Motive und Entwicklungstendenzen charakterisiert, die zu unterschiedlichen Zeitpunkten jeweils richtungsgebend auf die Entwicklung der Gemeinschaft eingewirkt haben.
Die Ausgestaltung und Fortführung des europäischen Integrationsprozesses bleibt auch unter den Bedingungen des Reformvertrags von Lissabon eine außerordentliche Bewährungsprobe.
Ihrem Vorschlag gemäß sollten Umweltpolitik, Einwanderung und Asylrecht, Gesundheit und Drogenbekämpfung vergemeinschaftet werden, eine europäische Staatsbürgerschaft eingeführt und eine Gemeinsame Außen- und Sicherheitspolitik (GASP) auf den Weg gebracht werden.
Erst nachdem die Dänen infolge Berücksichtigung gewisser Sonderinteressen (u.a. Nichtbeteiligung an der Währungsunion) in einer zweiten Volksabstimmung den Maastrichter Vertrag hatten passieren lassen und das deutsche Bundesverfassungsgericht Klagen gegen die Übertragung von Souveränitätsrechten auf die EU als  im gegebenen Rahmen  grundgesetzkonform zurückgewiesen hatte, konnte er zum 1. November 1993 in Kraft treten.
Bald danach  zum 1. Januar 1995  traten mit Österreich, Schweden und Finnland nach zügigen Beitrittsverhandlungen drei Staaten der EU bei, die bis zum Ende der Ost-West-Konfrontation durch ihre strikte Neutralitätspolitik daran gehindert waren.
Mehr als die erst bei wenigen ins Bewusstsein gedrungene Unionsbürgerschaft hat  bereits vor dem Euro  der Abbau der Grenzkontrollen und Grenzanlagen zwischen den Bürgern der am Schengener Abkommen beteiligten Mitgliedstaaten ein Gefühl europäischer Zusammengehörigkeit wecken können.
Die als Bestandteil des Binnenmarkts in der Einheitlichen Europäischen Akte festgeschriebene Freizügigkeit des Personenverkehrs wurde durch das 1985 in Schengen getroffene Abkommen zunächst von den Beneluxstaaten, Frankreich und der Bundesrepublik Deutschland auf den Weg gebracht, ohne dass aber die dazu nötige polizeiliche Zusammenarbeit und Vereinheitlichung der Visa die Durchführung schon gestattet hätte.
Kohl war es, der Mitte 1988 Jacques Delors für die Projektleitung vorgeschlagen hatte; dieser wiederum hatte die Mitwirkung der Chefs der europäischen Zentralbanken an der Entwicklung entsprechender Pläne durchgesetzt, um den geballten Sachverstand der je obersten Währungshüter gegen zu erwartende Widerstände einzelner Regierungen ins Feld führen zu können.
Wesentlich durch Kings Einsatz und Wirkkraft ist das Civil Rights Movement zu einer Massenbewegung geworden, die schließlich erreicht hat, dass die Rassentrennung gesetzlich aufgehoben und das uneingeschränkte Wahlrecht für die schwarze Bevölkerung der US-Südstaaten eingeführt wurde.
Der Vater änderte beide Namen nach einer Europareise im Jahre 1934, die ihn im Zusammenhang mit dem in Berlin stattfindenden baptistischen Weltkongress auch nach Deutschland führte, zu Ehren von Martin Luther, für den er große Bewunderung empfand.
Am 20. September 1944 begann King sein Studium am Morehouse College, der einzigen Hochschule für Schwarze im Süden; es nahm ihn trotz seines Alters von noch nicht 16 Jahren als Ausnahme auf.
Im Hauptfach Soziologie wurde er von Walter P. Chivers in die Problematik der Rassentrennung eingeführt; bei George D. Kelsey, dem Leiter der School of Religion, hörte er von Mahatma Gandhis gewaltfreiem Widerstand.