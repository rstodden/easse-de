{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reproduction based on Checkpoints\n",
    "\n",
    "We regenerate the outputs of TS systems for which a public checkpoint exist on huggingface, i.e., \n",
    "- https://huggingface.co/DEplain/trimmed_mbart_sents_apa,\n",
    "- https://huggingface.co/DEplain/trimmed_mbart_sents_apa_web, and\n",
    "- https://huggingface.co/josh-oo/custom-decoder-ats\n",
    "\n",
    "For the first models, we use the huggingface pipeline \"text2text-generation\". \n",
    "For the last model, we followed the instructions in the model repo. We added truncation to max_length of 1024, as some test sets records are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def generate_predictions(pipe, test_name_path, model_name, test_set_path=\"\", sys_out_path=\"\"):\n",
    "    print(test_set_path+test_name_path)\n",
    "    test_dataset = load_dataset(\"csv\", sep=\"\\t\", column_names=[\"original\"],  data_files={\"test\": test_set_path+test_name_path})\n",
    "    test_name_elements = test_name_path.split(\"/\")\n",
    "    test_name = test_name_elements[0]\n",
    "    # test_file \n",
    "    predictions = list()\n",
    "    for out in pipe(KeyDataset(test_dataset[\"test\"], \"original\")):\n",
    "        predictions.append(out[0][\"generated_text\"])\n",
    "    if not os.path.exists(sys_out_path+test_name+\"/test/\"):\n",
    "        os.makedirs(sys_out_path+test_name+\"/test/\")\n",
    "    print(test_name, model_name, sys_out_path+test_name+\"/test/\"+model_name+\".txt\")\n",
    "    # pd.DataFrame(predictions).to_csv(sys_out_path+test_name+\"/test/\"+model_name+\".txt\", index=None, header=None)\n",
    "    with open(sys_out_path+test_name+\"/test/\"+model_name+\".txt\", 'w') as f:\n",
    "        for line in predictions:\n",
    "            f.write(\"%s\\n\" % line)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response(original, params, tokenizer, model):\n",
    "\tinput_text = params[\"task_prefix\"]+original\n",
    "\tfeatures = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "\toutput = model.generate(input_ids=features['input_ids'],\t# .to(device), \n",
    "\t\t\t\t\t\t\t attention_mask=features['attention_mask'],\t# .to(device),\n",
    "\t\t\t\t\t\t\t max_length=params[\"max_target_length\"])\n",
    "\n",
    "\treturn tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_name_easse = \"/home/SSD1TB/easse-de/easse/resources/data/\"\n",
    "base_name_easse = \"../../resources/data/\"  # edit path your directory\n",
    "test_set_path = base_name_easse+\"test_sets/sentence_level/\"\n",
    "system_out_path = base_name_easse+\"system_outputs/sentence_level/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets_sentence_level = [\n",
    "                        \"TextComplexityDE/TextComplexityDE_test.org\", \n",
    "                        \"ZEST/geolino.test.org\", \n",
    "                        \"BiSECT/BiSECT_test.org\", \n",
    "                        \"DEplain-web/manual-public/DEplain-web-manual-public.test.org\", \n",
    "                        \"DEplain-APA/DEplain-APA.test.org\", # available upon request\n",
    "                        \"simple-german-corpus/simple-german-corpus_test.org\", # preprocessing required\n",
    "                        \"APA_LHAor-a2/APA_LHAor-a2_test.org\", # available upon request\n",
    "                        \"APA_LHAor-b1/APA_LHAor-b1_test.org\",  # available upon request\n",
    "                        \"ABGB/ABGB_test.org\",\n",
    "                        ## \"DEplain-APA-ref/DEplain-APA-ref_test.org\",\n",
    "                        ## \"DEplain-web-ref/DEplain-web-ref_test.org\",\n",
    "                        ## \"hda_easy_to_read_language/hda_easy_to_read_language_test.org\"\n",
    "                        \n",
    "                    ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "models = [\"DEplain/trimmed_mbart_sents_apa_web\", \"DEplain/trimmed_mbart_sents_apa\"]\n",
    "\n",
    "for model_name in models:\n",
    "    pipe = pipeline(\"text2text-generation\", model=model_name, device=0, trust_remote_code=True)\n",
    "    model_name_out = model_name.replace(\"/\", \"_\")\n",
    "    for test_data in test_sets_sentence_level:\n",
    "        print(test_data)\n",
    "        generate_predictions(pipe=pipe, test_name_path=test_data, model_name=model_name_out, \n",
    "                             test_set_path=test_set_path, sys_out_path=system_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"DEplain/trimmed_mbart_sents_apa_web\", \n",
    "          \"DEplain/trimmed_mbart_sents_apa\",\n",
    "          \"DEplain/mt5-DEplain-APA\",\n",
    "          \"DEplain/mt5-simple-german-corpus\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models:\n",
    "    model_name_out = model_name.replace(\"/\", \"_\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    for test_name_path in test_sets_sentence_level:\n",
    "        predictions = list()\n",
    "        test_df = pd.read_csv(test_set_path+test_name_path, sep=\"\\t\",header=None,names=[\"original\"])\n",
    "        test_name_elements = test_name_path.split(\"/\")\n",
    "        test_name = test_name_elements[0]\n",
    "        for i,sent in enumerate(test_df[\"original\"]):\n",
    "            if not i%100:\n",
    "                print(test_name_path, i)\n",
    "            # print(sent)\n",
    "            if \"mt5-\" in model_name_out:\n",
    "                prefix = \"simplify to plain German: \"\n",
    "            else:\n",
    "                prefix = \"\"\n",
    "            inputs = tokenizer([prefix+sent], return_tensors=\"pt\")\n",
    "            outputs = model.generate(**inputs, max_length=128)\n",
    "            predictions.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "        if not os.path.exists(sys_out_path+test_name+\"/test/\"):\n",
    "            os.makedirs(sys_out_path+test_name+\"/test/\")\n",
    "        with open(sys_out_path+test_name+\"/test/\"+model_name_out+\".txt\", 'w') as f:\n",
    "            for line in predictions:\n",
    "                f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/custom-decoder-ats\")\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"josh-oo/custom-decoder-ats\", trust_remote_code=True, revision=\"4accedbe0b57d342d95ff546b6bbd3321451d504\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/german-gpt2-easy\")\n",
    "decoder_tokenizer.add_tokens(['<</s>>','<<s>>','<<pad>>'])\n",
    "model_name_out = \"josh-oo/custom-decoder-ats\".replace(\"/\", \"_\")\n",
    "##\n",
    "\n",
    "# example_text = \"In tausenden Schweizer Privathaushalten kümmern sich Haushaltsangestellte um die Wäsche, betreuen die Kinder und sorgen für Sauberkeit. Durchschnittlich bekommen sie für die Arbeit rund 30 Franken pro Stunde Bruttolohn. Der grösste Teil von ihnen erhält aber 28 Franken.\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for test_data in test_sets_sentence_level:\n",
    "    test_dataset = load_dataset(\"csv\", sep=\"\\t\", column_names=[\"original\"],  data_files={\"test\": test_set_path+test_data})\n",
    "    test_name_elements = test_data.split(\"/\")\n",
    "    test_name = test_name_elements[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    predictions = list()\n",
    "    for text in test_dataset[\"test\"][\"original\"]:\n",
    "        test_input = tokenizer([text], return_tensors=\"pt\", padding=True, pad_to_multiple_of=1024)\n",
    "        for key, value in test_input.items():\n",
    "          test_input[key] = value.to(device)\n",
    "        output = model.generate(**test_input, num_beams=3, max_length=1024)\n",
    "        prediction = decoder_tokenizer.batch_decode(output)\n",
    "        predictions.append(prediction)\n",
    "    with open(system_out_path+test_name+\"/test/\"+model_name_out+\".txt\", 'w') as f:\n",
    "            for line in predictions:\n",
    "                f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_out = \"josh-oo/custom-decoder-ats\".replace(\"/\", \"_\")\n",
    "for test_data in test_sets_sentence_level:\n",
    "    test_dataset = load_dataset(\"csv\", sep=\"\\t\", column_names=[\"original\"],  data_files={\"test\": test_set_path+test_data})\n",
    "    test_name_elements = test_data.split(\"/\")\n",
    "    if \"DEplain-web\" in test_name_elements:\n",
    "        test_name = \"_\".join(test_name_elements[0:2])\n",
    "    else:\n",
    "        test_name = test_name_elements[0]\n",
    "    result_files = [name for name in os.listdir(system_out_path+test_name+\"/test/\") if model_name_out in name]\n",
    "    print(result_files)\n",
    "    for filename in result_files:\n",
    "        with open(system_out_path+test_name+\"/test/\"+filename, 'r') as f:\n",
    "            content = f.read()\n",
    "        print(system_out_path+test_name+\"/test/\"+filename+\"_clean.txt\")\n",
    "        with open(system_out_path+test_name+\"/test/\"+filename+\"_clean.txt\", 'w') as f:\n",
    "            content = content.replace(\"['<s>\", \"\")\n",
    "            content = content.replace(\"<s>\", \"\")\n",
    "            content = content.replace('\"[<s>', \"\")\n",
    "            content = content.replace(\"<</s>>']\", \"\")\n",
    "            content = content.replace(\"</s>']\", \"\")\n",
    "            content = content.replace('<</s>>\"]', \"\")\n",
    "            content = content.replace(\"</s>']\", \"\")\n",
    "            content = content.replace(\"</s>\", \"\")\n",
    "            \n",
    "            \n",
    "            f.write(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easse-de/easse/resources/data/system_outputs/sentence_level/DEplain-web/test/josh-oo_custom-decoder-ats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Level Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name_easse = \"../../resources/data/\"\n",
    "test_set_path = base_name_easse+\"test_sets/document_level/\"\n",
    "system_out_path = base_name_easse+\"system_outputs/document_level/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets_document_level = [\n",
    "                      \"20Minuten/20Minuten_test.org\", \n",
    "                      \"DEplain-APA/DEplain-APA.test.org\",\n",
    "                      ## \"hda_easy_to_read_language/hda_easy_to_read_langauge_test.org\",\n",
    "                      \"klexikon/klexikon_test.org\",\n",
    "                      \"DEplain-web/auto-public/DEplain-web-auto-public.test.org\",\n",
    "                      \"DEplain-web/manual-public/DEplain-web-manual-public.test.org\",\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "##gerpt\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"josh-oo/custom-decoder-ats\", trust_remote_code=True, revision=\"35197269f0235992fcc6b8363ca4f48558b624ff\")\n",
    "#decoder_tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/gerpt2\")\n",
    "\n",
    "##dbmdz\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/custom-decoder-ats\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"josh-oo/custom-decoder-ats\", trust_remote_code=True, revision=\"4accedbe0b57d342d95ff546b6bbd3321451d504\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"josh-oo/german-gpt2-easy\")\n",
    "decoder_tokenizer.add_tokens(['<</s>>','<<s>>','<<pad>>'])\n",
    "model_name_out = \"josh-oo/custom-decoder-ats\".replace(\"/\", \"_\")\n",
    "##\n",
    "\n",
    "# example_text = \"In tausenden Schweizer Privathaushalten kümmern sich Haushaltsangestellte um die Wäsche, betreuen die Kinder und sorgen für Sauberkeit. Durchschnittlich bekommen sie für die Arbeit rund 30 Franken pro Stunde Bruttolohn. Der grösste Teil von ihnen erhält aber 28 Franken.\"\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for test_data in test_sets_document_level:\n",
    "    test_dataset = load_dataset(\"csv\", sep=\"\\t\", column_names=[\"original\"],  data_files={\"test\": test_set_path+test_data})\n",
    "    test_name_elements = test_data.split(\"/\")\n",
    "    if \"DEplain-web\" in test_name_elements:\n",
    "        test_name = \"_\".join(test_name_elements[0:2])\n",
    "    else:\n",
    "        test_name = test_name_elements[0]\n",
    "    print(test_data, len(test_dataset[\"test\"]))\n",
    "    # print(test_dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    predictions = list()\n",
    "    for text in test_dataset[\"test\"][\"original\"]:\n",
    "        # test_input = tokenizer([text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1024)  # pad_to_multiple_of=1024, \n",
    "        # test_input = tokenizer([text], return_tensors=\"pt\", padding=True, pad_to_multiple_of=1024,)\n",
    "        test_input = tokenizer([text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=4096)\n",
    "        # print(test_input[\"input_ids\"].shape[1])\n",
    "        print(len(text), len(text.split(\" \")), test_input[\"input_ids\"].shape)\n",
    "        # test_input = tokenizer([text], return_tensors=\"pt\", padding=True, pad_to_multiple_of=1024, truncation=True, max_length=test_input[\"input_ids\"].shape[1])\n",
    "\n",
    "        for key, value in test_input.items():\n",
    "          test_input[key] = value.to(device)\n",
    "        # print(len(text), len(text.split(\" \")), test_input[\"input_ids\"].shape)\n",
    "        output = model.generate(**test_input, num_beams=3, max_length=1024)\n",
    "        prediction = decoder_tokenizer.batch_decode(output)\n",
    "        predictions.append(prediction)\n",
    "    if not os.path.exists(system_out_path+test_name+\"/test/\"):\n",
    "        os.makedirs(system_out_path+test_name+\"/test/\")\n",
    "                          \n",
    "    with open(system_out_path+test_name+\"/test/\"+model_name_out+\"_trunc_4096.txt\", 'w') as f:\n",
    "            for line in predictions:\n",
    "                f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_out = \"josh-oo/custom-decoder-ats\".replace(\"/\", \"_\")\n",
    "for test_data in test_sets_document_level:\n",
    "    test_dataset = load_dataset(\"csv\", sep=\"\\t\", column_names=[\"original\"],  data_files={\"test\": test_set_path+test_data})\n",
    "    test_name_elements = test_data.split(\"/\")\n",
    "    if \"DEplain-web\" in test_name_elements:\n",
    "        test_name = \"_\".join(test_name_elements[0:2])\n",
    "    else:\n",
    "        test_name = test_name_elements[0]\n",
    "    result_files = [name for name in os.listdir(system_out_path+test_name+\"/test/\") if model_name_out in name]\n",
    "    for filename in result_files:\n",
    "        with open(system_out_path+test_name+\"/test/\"+filename, 'r') as f:\n",
    "            content = f.read()\n",
    "        with open(system_out_path+test_name+\"/test/\"+filename+\"_clean.txt\", 'w') as f:\n",
    "            content = content.replace(\"['<s>\", \"\")\n",
    "            content = content.replace(\"<s>\", \"\")\n",
    "            content = content.replace('\"[<s>', \"\")\n",
    "            content = content.replace(\"<</s>>']\", \"\")\n",
    "            content = content.replace(\"</s>']\", \"\")\n",
    "            content = content.replace('<</s>>\"]', \"\")\n",
    "            content = content.replace(\"</s>']\", \"\")\n",
    "            content = content.replace(\"</s>\", \"\")\n",
    "            f.write(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
